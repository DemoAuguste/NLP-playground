{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21bf3277690>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[-0.0187,  0.1713, -0.2944]],\n\n        [[-0.3521,  0.1026, -0.2971]],\n\n        [[-0.3191,  0.0781, -0.1957]],\n\n        [[-0.1634,  0.0941, -0.1637]],\n\n        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward>)\n(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
    "\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n"
     ]
    }
   ],
   "source": [
    "character_to_ix = {}\n",
    "for i in range(26):\n",
    "    character_to_ix[chr(ord('a')+i)] = i\n",
    "print(character_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.1389, -1.2024, -0.9693],\n",
      "        [-1.1065, -1.2200, -0.9834],\n",
      "        [-1.1286, -1.2093, -0.9726],\n",
      "        [-1.1190, -1.1960, -0.9916],\n",
      "        [-1.0137, -1.2642, -1.0366]])\n",
      "tensor([[-0.0462, -4.0106, -3.6096],\n",
      "        [-4.8205, -0.0286, -3.9045],\n",
      "        [-3.7876, -4.1355, -0.0394],\n",
      "        [-0.0185, -4.7874, -4.6013],\n",
      "        [-5.7881, -0.0186, -4.1778]])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        # print(tag_scores, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_process_data(seq, tags, tag_to_idx):\n",
    "    global character_to_ix\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    for word, tag in zip(seq, tags):\n",
    "        embed_input = [character_to_ix[i] for i in word.lower()]\n",
    "        embed_target = [tag_to_ix[tag] for _ in range(len(embed_input))]\n",
    "        input_list.append(embed_input)\n",
    "        target_list.append(embed_target)\n",
    "    return input_list, target_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.3427, -0.8317, -1.1921],\n        [-1.3340, -0.7787, -1.2816],\n        [-1.3858, -0.8243, -1.1669]])\ntensor([[-1.3221, -0.9552, -1.0536],\n        [-1.2236, -0.8827, -1.2304],\n        [-1.3707, -0.9384, -1.0361]])\ntensor([[-1.4397, -0.8951, -1.0373],\n        [-1.4099, -0.8532, -1.1093],\n        [-1.4215, -0.8685, -1.0815]])\ntensor([[-1.3427, -0.8317, -1.1921],\n        [-1.3340, -0.7787, -1.2816],\n        [-1.3858, -0.8243, -1.1669]])\ntensor([[-1.4397, -0.8951, -1.0373],\n        [-1.3763, -0.8679, -1.1157],\n        [-1.3289, -0.7994, -1.2530],\n        [-1.3699, -0.7810, -1.2451],\n        [-1.3579, -0.8126, -1.2070]])\n"
     ]
    }
   ],
   "source": [
    "model2 = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(character_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    input_list, target_list = character_process_data(training_data[0][0], training_data[0][1], tag_to_ix)\n",
    "    # print(input_list)\n",
    "    # print(target_list)\n",
    "    # inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    for inp in input_list:\n",
    "        tag_scores = model2(torch.tensor(inp))\n",
    "        print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model2.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        input_list, target_list = character_process_data(training_data[0][0], training_data[0][1], tag_to_ix)\n",
    "\n",
    "        for inp, target in zip(input_list, target_list):\n",
    "            tag_scores = model2(torch.tensor(inp))\n",
    "            # print(torch.tensor(inp))\n",
    "            # print(torch.tensor(target))\n",
    "            loss = loss_function(tag_scores, torch.tensor(target))\n",
    "            loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-5.0838e-03, -5.9908e+00, -5.9641e+00],\n        [-8.6151e-04, -7.5420e+00, -8.0143e+00],\n        [-7.5776e-04, -7.4850e+00, -8.5369e+00]])\ntensor([[-7.0276e+00, -3.2161e-03, -6.0645e+00],\n        [-7.4925e+00, -1.9047e-03, -6.6109e+00],\n        [-1.0315e+01, -6.6783e-04, -7.3627e+00]])\ntensor([[-6.4776e+00, -9.8602e-01, -4.6936e-01],\n        [-5.5515e+00, -5.5390e+00, -7.8427e-03],\n        [-5.9457e+00, -5.8968e+00, -5.3797e-03]])\ntensor([[-5.0838e-03, -5.9908e+00, -5.9641e+00],\n        [-8.6151e-04, -7.5420e+00, -8.0143e+00],\n        [-7.5776e-04, -7.4850e+00, -8.5369e+00]])\ntensor([[-6.4776e+00, -9.8602e-01, -4.6936e-01],\n        [-7.9744e+00, -5.8629e-03, -5.2027e+00],\n        [-7.6790e+00, -1.2270e-03, -7.1772e+00],\n        [-8.0469e+00, -7.8349e-04, -7.6776e+00],\n        [-9.8114e+00, -7.3513e-04, -7.2934e+00]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_list, target_list = character_process_data(training_data[0][0], training_data[0][1], tag_to_ix)\n",
    "    # print(input_list)\n",
    "    # print(target_list)\n",
    "    # inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    for inp in input_list:\n",
    "        tag_scores = model2(torch.tensor(inp))\n",
    "        print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-5.0838e-03, -5.9908e+00, -5.9641e+00],\n        [-8.6151e-04, -7.5420e+00, -8.0143e+00],\n        [-7.5776e-04, -7.4850e+00, -8.5369e+00]])\ntensor([[-0.0361, -4.1057, -3.9661],\n        [-0.0402, -3.3201, -5.7278],\n        [-0.4549, -1.0099, -6.7037]])\ntensor([[-2.7923e+00, -6.4537e-02, -6.7114e+00],\n        [-4.5291e+00, -1.1228e-02, -7.8887e+00],\n        [-6.9707e+00, -1.8566e-02, -4.0481e+00],\n        [-7.5370e+00, -3.6140e-03, -5.7846e+00],\n        [-7.3521e+00, -1.1166e-03, -7.6526e+00]])\ntensor([[-5.0838e-03, -5.9908e+00, -5.9641e+00],\n        [-8.6151e-04, -7.5420e+00, -8.0143e+00],\n        [-7.5776e-04, -7.4850e+00, -8.5369e+00]])\ntensor([[-3.6068e-02, -4.1057e+00, -3.9661e+00],\n        [-4.2641e+00, -1.8467e+00, -1.8853e-01],\n        [-6.2282e+00, -4.3152e-02, -3.2124e+00],\n        [-8.3039e+00, -2.1538e-03, -6.2638e+00]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_list, target_list = character_process_data(['The', 'boy', 'plays', 'the', 'ball'], [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"], tag_to_ix)\n",
    "    # print(input_list)\n",
    "    # print(target_list)\n",
    "    # inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    for inp in input_list:\n",
    "        tag_scores = model2(torch.tensor(inp))\n",
    "        print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}